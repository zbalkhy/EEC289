{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5647cea",
   "metadata": {},
   "source": [
    "# Assignment 2 — Starter Notebook\n",
    "\n",
    "Fill in the TODO sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "153ede56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "%matplotlib qt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1856a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(imgs, nrow=8, title=\"\", cmap=None, resize_to=None):\n",
    "    import math\n",
    "    import torchvision\n",
    "    import torch.nn.functional as F\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        return\n",
    "\n",
    "    proc = []\n",
    "    shapes = []\n",
    "    for x in imgs:\n",
    "        x = x.detach().cpu()\n",
    "        if x.ndim == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        proc.append(x)\n",
    "        shapes.append((int(x.shape[1]), int(x.shape[2])))\n",
    "\n",
    "    if resize_to is None and len(set(shapes)) > 1:\n",
    "        resize_to = (min(h for h, w in shapes), min(w for h, w in shapes))\n",
    "\n",
    "    if resize_to is not None:\n",
    "        th, tw = int(resize_to[0]), int(resize_to[1])\n",
    "        resized = []\n",
    "        for x in proc:\n",
    "            if int(x.shape[1]) == th and int(x.shape[2]) == tw:\n",
    "                resized.append(x)\n",
    "                continue\n",
    "            xb = x.unsqueeze(0)\n",
    "            mode = \"nearest\" if x.shape[0] == 1 else \"bilinear\"\n",
    "            xb = F.interpolate(xb, size=(th, tw), mode=mode, align_corners=False if mode == \"bilinear\" else None)\n",
    "            resized.append(xb.squeeze(0))\n",
    "        proc = resized\n",
    "\n",
    "    grid = torchvision.utils.make_grid(proc, nrow=nrow, padding=2)\n",
    "    grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "    plt.figure(figsize=(nrow * 1.2, max(1, math.ceil(len(imgs) / nrow)) * 1.2))\n",
    "    if grid.shape[2] == 1:\n",
    "        plt.imshow(grid[:, :, 0], cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(grid)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31886558",
   "metadata": {},
   "source": [
    "## Textures (provided)\n",
    "\n",
    "Download the provided `textures.zip` from **Canvas → Files**, then upload it to Colab (Files panel).\n",
    "Keep the filename as **`textures.zip`**. The next cell will extract images into `/content/textures/`.\n",
    "\n",
    "Alternatively, you may upload individual texture images directly into `/content/textures/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f21c5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "def list_texture_files(texture_dir: Path):\n",
    "    exts = (\".png\", \".jpg\", \".jpeg\", \".bmp\")\n",
    "    return [p for p in sorted(texture_dir.iterdir()) if p.is_file() and p.suffix.lower() in exts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e442f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found texture images: 12\n",
      " - 1.png\n",
      " - 10.png\n",
      " - 11.png\n",
      " - 12.png\n",
      " - 2.png\n",
      " - 3.png\n",
      " - 4.png\n",
      " - 5.png\n",
      " - 6.png\n",
      " - 7.png\n",
      " - 8.png\n",
      " - 9.png\n"
     ]
    }
   ],
   "source": [
    "TEXTURE_DIR = Path(\"content/textures\")\n",
    "TEXTURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "texture_files = list_texture_files(TEXTURE_DIR)\n",
    "\n",
    "# If the folder is empty, extract from /content/textures.zip.\n",
    "if len(texture_files) == 0:\n",
    "    ZIP_PATH = Path(\"content/textures.zip\")\n",
    "    if not ZIP_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"No texture images found in /content/textures.\\n\"\n",
    "            \"Upload the provided texture archive and name it 'textures.zip', then re-run this cell.\"\n",
    "        )\n",
    "\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\", \".bmp\"}\n",
    "    counts = {}\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        for info in z.infolist():\n",
    "            if info.is_dir():\n",
    "                continue\n",
    "            name = info.filename\n",
    "            if \"__MACOSX\" in name:\n",
    "                continue\n",
    "            suf = Path(name).suffix.lower()\n",
    "            if suf not in exts:\n",
    "                continue\n",
    "            base = Path(name).name  # flatten\n",
    "            counts.setdefault(base, 0)\n",
    "            out = TEXTURE_DIR / base\n",
    "            if counts[base] > 0:\n",
    "                out = TEXTURE_DIR / f\"{Path(base).stem}_{counts[base]}{suf}\"\n",
    "            counts[base] += 1\n",
    "            with z.open(info) as src, open(out, \"wb\") as dst:\n",
    "                dst.write(src.read())\n",
    "\n",
    "    texture_files = list_texture_files(TEXTURE_DIR)\n",
    "\n",
    "print(\"Found texture images:\", len(texture_files))\n",
    "for p in texture_files[:20]:\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "if len(texture_files) == 0:\n",
    "    raise RuntimeError(\"textures.zip extracted no images; check the archive contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "59af935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_texture_as_tensor(path: Path, resize: int | None = None):\n",
    "    \"\"\"Return (C,H,W) float tensor in [0,1].\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    if resize is not None:\n",
    "        img = img.resize((resize, resize), Image.BILINEAR)\n",
    "    x = torch.from_numpy(np.array(img)).float() / 255.0\n",
    "    x = x.permute(2, 0, 1).contiguous()\n",
    "    return x\n",
    "\n",
    "def extract_patches_from_image(img: torch.Tensor, window: int, max_patches: int | None = None, device: torch.device = device):\n",
    "    \"\"\"img: (C,H,W) -> patches: (N,C,window,window).\"\"\"\n",
    "    assert img.ndim == 3\n",
    "    C, H, W = img.shape\n",
    "    assert window <= H and window <= W\n",
    "    unfold = torch.nn.Unfold(kernel_size=window, padding=0, stride=1)\n",
    "    patches = unfold(img.unsqueeze(0))\n",
    "    patches = patches.transpose(1, 2).reshape(-1, C, window, window)\n",
    "    if max_patches is not None and patches.shape[0] > max_patches:\n",
    "        idx = torch.randperm(patches.shape[0])[:max_patches]\n",
    "        patches = patches[idx]\n",
    "    return patches.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c5106103",
   "metadata": {},
   "outputs": [],
   "source": [
    "texture = load_texture_as_tensor(texture_files[0])\n",
    "texture_patches = extract_patches_from_image(texture, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "637426c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([276676, 3, 5, 5])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texture_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "a7d9e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frontier(known: torch.Tensor, valid: torch.Tensor, neighborhood: int = 8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    known, valid: (H,W) bool\n",
    "    frontier: unknown & valid & adjacent-to-known\n",
    "    neighborhood: 4 or 8\n",
    "\n",
    "    Implementation hint:\n",
    "      - Use a 3x3 convolution over the known-mask to count known neighbors.\n",
    "      - For 8-neighborhood: use a 3x3 all-ones kernel with center = 0.\n",
    "      - For 4-neighborhood: use a cross-shaped kernel.\n",
    "\n",
    "    Return:\n",
    "      frontier: (H,W) bool\n",
    "    \"\"\"\n",
    "    assert known.shape == valid.shape\n",
    "    H, W = known.shape\n",
    "    known_f = known.float().view(1, 1, H, W)\n",
    "    if neighborhood == 8:\n",
    "        # TODO: 3x3 all-ones kernel with center = 0, on the same device as `known`\n",
    "        k = torch.ones((3,3), device=known.device, dtype=torch.float32)\n",
    "        k[1,1] = 0\n",
    "        k = k.view(1,1,3,3)\n",
    "    elif neighborhood == 4:\n",
    "        # TODO: 3x3 cross-shaped kernel, on the same device as `known`\n",
    "        k = torch.tensor(\n",
    "            [[0, 1, 0],\n",
    "            [1, 1, 1],\n",
    "            [0, 1, 0]], device=known.device, dtype=torch.float32).view(1,1,3,3)\n",
    "    else:\n",
    "        raise ValueError(\"neighborhood must be 4 or 8\")\n",
    "\n",
    "    # Count known neighbors at each pixel\n",
    "    nb = F.conv2d(known_f, k, padding=1).view(H, W)\n",
    "    # Frontier = unknown pixels that are valid and have at least one known neighbor\n",
    "    frontier = (~known) & valid & (nb > 0)\n",
    "    return frontier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "07480ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_seed_patch_index(patches: torch.Tensor, seed_fg_min: int = 0) -> int:\n",
    "    \"\"\"\n",
    "    Choose an index into the patch pool for the initial seed.\n",
    "\n",
    "    If seed_fg_min > 0, prefer patches whose total foreground mass (sum over all elements)\n",
    "    is at least seed_fg_min. If no patch satisfies the constraint, fall back to uniform random.\n",
    "\n",
    "    TODO:\n",
    "      - Implement the logic described above.\n",
    "      - Return a Python int index.\n",
    "    \"\"\"\n",
    "    assert seed_fg_min >= 0\n",
    "    if seed_fg_min == 0:\n",
    "        return torch.randint(0,patches.shape[0],(1,)).item()\n",
    "    else:\n",
    "        #TODO compute sum over all elements, this is weird i dont get it.\n",
    "        return 0\n",
    "\n",
    "\n",
    "def choose_frontier_pixel(\n",
    "    frontier: torch.Tensor,   # (H,W) bool\n",
    "    known: torch.Tensor,      # (H,W) bool\n",
    "    ones_w: torch.Tensor,     # (1,1,w,w) float\n",
    "    pad: int,\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Pick a pixel (y,x) from the frontier with the *maximum* number of known pixels\n",
    "    in its w×w neighborhood window. Break ties uniformly at random.\n",
    "\n",
    "      - Use a conv2d with kernel ones_w to compute known_count map.\n",
    "      - Restrict to pixels where frontier==True.\n",
    "      - Return (y,x) as Python ints.\n",
    "    \"\"\"\n",
    "    H, W = known.shape\n",
    "    known_f = known.float().view(1, 1, H, W)\n",
    "    known_nb = F.conv2d(known_f, ones_w, padding='same').view(H,W)\n",
    "\n",
    "    valid_pixels = (known_nb > 0) & frontier\n",
    "    valid_pixel_nb = valid_pixels*known_nb\n",
    "    \n",
    "    max_val = torch.max(valid_pixel_nb)\n",
    "    max_mask = (valid_pixel_nb == max_val)\n",
    "    indices_of_max = max_mask.nonzero()\n",
    "    choice = torch.randint(0,indices_of_max.shape[0], (1,)).item()\n",
    "    return indices_of_max[choice]\n",
    "\n",
    "\n",
    "def masked_ssd(\n",
    "    tgt_patch: torch.Tensor,     # (C,w,w)\n",
    "    patches_flat: torch.Tensor,  # (N,C,w*w)\n",
    "    mask: torch.Tensor,          # (w*w) bool\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute masked SSD distance between a target patch and each candidate patch.\n",
    "\n",
    "      - Flatten tgt_patch to (C,w*w) and keep only masked entries -> (C,K)\n",
    "      - Select the same masked entries from patches_flat -> (N,C,K)\n",
    "      - Return dist: (N,) where dist[n] = sum_{c,k} (cand - tgt)^2\n",
    "    \"\"\"\n",
    "    \n",
    "    tgt_flat = tgt_patch.reshape(tgt_patch.shape[0], -1)\n",
    "    masked_patches = patches_flat*mask\n",
    "\n",
    "    return torch.linalg.vector_norm((patches_flat - tgt_flat), ord=2, dim=2).sum(dim=1)\n",
    "\n",
    "\n",
    "def candidate_set(dist: torch.Tensor, eps: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Candidate set rule from Efros & Leung:\n",
    "      { i | dist[i] <= (1+eps) * dmin }, where dmin = min(dist)\n",
    "\n",
    "      - Compute dmin (a scalar tensor)\n",
    "      - Return (cand_idx, dmin), where cand_idx is 1D LongTensor of indices\n",
    "      - Handle any edge cases (e.g., empty candidate set)\n",
    "    \"\"\"\n",
    "    dmin = dist.min()\n",
    "    candidate_mask = (dist <= (1+eps) * dmin)\n",
    "    candidate_indices = candidate_mask.nonzero()\n",
    "    return (candidate_indices, dmin)\n",
    "\n",
    "\n",
    "def sample_from_candidates(\n",
    "    cand_idx: torch.Tensor,   # (M,)\n",
    "    dist: torch.Tensor,       # (N,)\n",
    "    dmin: torch.Tensor,       # scalar\n",
    "    weighted: bool = True,\n",
    "    h_mult: float = 0.3,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Sample one candidate index from cand_idx.\n",
    "\n",
    "    If weighted=True, sample with probability proportional to\n",
    "        exp( - (dist[i] - dist_min_over_cands) / (h_mult*dmin + 1e-6) ).\n",
    "\n",
    "    Otherwise, sample uniformly from cand_idx.\n",
    "      - Implement both sampling modes.\n",
    "      - Return the *chosen* index as a Python int.\n",
    "    \"\"\"\n",
    "    if weighted:\n",
    "        candidates = dist[cand_idx]\n",
    "        dist_min_over_cands = torch.min(candidates)\n",
    "        weights = torch.exp(-(candidates - dist_min_over_cands) / (h_mult * dmin + 1e-6))\n",
    "        # Normalize weights to sum to 1\n",
    "        weights = weights / weights.sum()\n",
    "        # Sample from the candidates using multinomial\n",
    "        sampled_idx = torch.multinomial(weights.view(-1), 1)\n",
    "        return cand_idx[sampled_idx].item()\n",
    "    else:\n",
    "        # sample uniformly\n",
    "        return cand_idx[torch.randint(0, cand_idx.shape[0], (1,)).item()].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "e4d86704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def efros_leung_synthesize_from_patch_pool(\n",
    "    patches: torch.Tensor,  # (N,C,w,w)\n",
    "    out_h: int,\n",
    "    out_w: int,\n",
    "    eps: float = 0.05,\n",
    "    neighborhood: int = 8,\n",
    "    seed_fg_min: int = 0,\n",
    "    border: int = 0,\n",
    "    weighted: bool = True,\n",
    "    h_mult: float = 0.3,\n",
    "    verbose: bool = False,\n",
    "    return_stats: bool = False,\n",
    "    max_steps: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Efros–Leung synthesis using a pre-built patch pool.\n",
    "\n",
    "    Core algorithmic requirements (you must implement):\n",
    "      - Initialize output with a random seed patch.\n",
    "      - Maintain a frontier set (unknown pixels adjacent to known).\n",
    "      - Pick a frontier pixel with the most known neighbors in its w×w window.\n",
    "      - Compute masked SSD between the target neighborhood and each candidate patch.\n",
    "      - Candidate set: d <= (1+eps)*dmin. Sample one candidate (uniform or weighted).\n",
    "      - Copy the *center pixel only* from the sampled candidate patch.\n",
    "\n",
    "    Args:\n",
    "      patches: (N,C,w,w) float in [0,1]\n",
    "      return_stats: if True, return (out, stats) where stats includes a match-error score\n",
    "\n",
    "    Return:\n",
    "      out: (C,out_h,out_w) on CPU if return_stats=False\n",
    "      (out, stats) if return_stats=True\n",
    "    \"\"\"\n",
    "    assert patches.ndim == 4\n",
    "    N, C, w, _ = patches.shape\n",
    "    assert w % 2 == 1, \"window size must be odd\"\n",
    "    pad = w // 2\n",
    "\n",
    "    # Precompute flattened patches and their center pixels\n",
    "    patches_flat = patches.reshape(N, C, -1)  # (N,C,w*w)\n",
    "    center = w // 2\n",
    "    center_pixels = patches[:, :, center, center]  # (N,C)\n",
    "\n",
    "    # pick the seed patch index\n",
    "    seed_idx = choose_seed_patch_index(patches, seed_fg_min)\n",
    "\n",
    "    # Allocate padded output and masks\n",
    "    H, W = int(out_h), int(out_w)\n",
    "    out = torch.zeros((C, H + 2 * pad, W + 2 * pad), device=patches.device, dtype=torch.float32)\n",
    "    known = torch.zeros((H + 2 * pad, W + 2 * pad), device=patches.device, dtype=torch.bool)\n",
    "    valid = torch.zeros_like(known)\n",
    "    valid[pad : pad + H, pad : pad + W] = True\n",
    "\n",
    "    # initialize with the seed patch at the center of the valid region\n",
    "    left_side = -w//2\n",
    "    right_side = w//2\n",
    "    cntr_pix = (pad+H//2, pad+W//2)\n",
    "    out[:,cntr_pix[0] + left_side:cntr_pix[0] + right_side, \n",
    "        cntr_pix[1] + left_side:cntr_pix[1] + right_side] = patches[seed_idx]\n",
    "    known[cntr_pix[0] + left_side:cntr_pix[0] + right_side,\n",
    "          cntr_pix[1] + left_side:cntr_pix[1] + right_side] = 1\n",
    "    #TODO remove plotting sanity check code \n",
    "    #plt.imshow(out.permute(1, 2, 0).numpy())\n",
    "    #plt.show()\n",
    "\n",
    "    # Optional known border (often useful on MNIST to avoid wandering)\n",
    "    if border > 0:\n",
    "        top, bottom = pad, pad + H\n",
    "        left, right = pad, pad + W\n",
    "        out[:, top : top + border, left:right] = 0.0\n",
    "        out[:, bottom - border : bottom, left:right] = 0.0\n",
    "        out[:, top:bottom, left : left + border] = 0.0\n",
    "        out[:, top:bottom, right - border : right] = 0.0\n",
    "\n",
    "        known[top : top + border, left:right] = True\n",
    "        known[bottom - border : bottom, left:right] = True\n",
    "        known[top:bottom, left : left + border] = True\n",
    "        known[top:bottom, right - border : right] = True\n",
    "\n",
    "    total_to_fill = int(valid.sum().item())\n",
    "    pbar = tqdm(total=total_to_fill, disable=not verbose)\n",
    "    pbar.update(int((known & valid).sum().item()))\n",
    "\n",
    "    ones_w = torch.ones((1, 1, w, w), device=patches.device, dtype=torch.float32)\n",
    "\n",
    "    dmin_hist = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        steps = 0\n",
    "        while int((known & valid).sum().item()) < total_to_fill:\n",
    "            if max_steps is not None and steps >= int(max_steps):\n",
    "                break\n",
    "\n",
    "            frontier = compute_frontier(known, valid, neighborhood=neighborhood)\n",
    "            if frontier.sum().item() == 0:\n",
    "                break\n",
    "\n",
    "            # select the next pixel to fill from the frontier\n",
    "            y, x = choose_frontier_pixel(frontier, known, ones_w, pad)\n",
    "\n",
    "            tgt_patch = out[:, y - pad : y + pad + 1, x - pad : x + pad + 1]\n",
    "            tgt_known = known[y - pad : y + pad + 1, x - pad : x + pad + 1].reshape(-1)\n",
    "            mask = tgt_known.bool()\n",
    "\n",
    "            if mask.sum().item() == 0:\n",
    "                # No known neighbors yet: fall back to a random center pixel\n",
    "                ridx = torch.randint(0, N, (1,), device=patches.device).item()\n",
    "                out[:, y, x] = center_pixels[ridx]\n",
    "                known[y, x] = True\n",
    "                pbar.update(1)\n",
    "                steps += 1\n",
    "                continue\n",
    "\n",
    "            # compute dist over all candidate patches using masked SSD\n",
    "            dist = masked_ssd(tgt_patch, patches_flat, mask)\n",
    "\n",
    "            # candidate-set selection\n",
    "            cand_idx, dmin = candidate_set(dist, eps)\n",
    "\n",
    "            # sample a candidate index from the candidate set\n",
    "            chosen = sample_from_candidates(cand_idx, dist, dmin)\n",
    "\n",
    "            out[:, y, x] = center_pixels[chosen]\n",
    "            known[y, x] = True\n",
    "            pbar.update(1)\n",
    "\n",
    "            dmin_hist.append(float(dmin.item()))\n",
    "            steps += 1\n",
    "\n",
    "    pbar.close()\n",
    "    out = out[:, pad : pad + H, pad : pad + W].detach().cpu()\n",
    "\n",
    "    if not return_stats:\n",
    "        return out\n",
    "\n",
    "    stats = {\n",
    "        \"steps\": steps,\n",
    "        \"filled\": int((known & valid).sum().item()),\n",
    "        \"avg_dmin\": float(np.mean(dmin_hist)) if len(dmin_hist) > 0 else float(\"nan\"),\n",
    "        \"dmin_hist\": dmin_hist,\n",
    "    }\n",
    "    return out, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "efros_leung_synthesize_from_patch_pool(texture_patches, 64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec7207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:55<00:00, 55.85s/it]\n",
      "  0%|          | 0/1 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[249]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tqdm(WINDOWS):\n\u001b[32m     13\u001b[39m     patches = extract_patches_from_image(src, window=w, max_patches=MAX_PATCHES, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     out = \u001b[43mefros_leung_synthesize_from_patch_pool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_h\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUT_H\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_w\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUT_W\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneighborhood\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNEIGHBORHOOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed_fg_min\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mborder\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweighted\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     show_grid([src.cpu(), out], nrow=\u001b[32m2\u001b[39m, resize_to=(OUT_H, OUT_W), title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | w=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[244]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mefros_leung_synthesize_from_patch_pool\u001b[39m\u001b[34m(patches, out_h, out_w, eps, neighborhood, seed_fg_min, border, weighted, h_mult, verbose, return_stats, max_steps)\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# compute dist over all candidate patches using masked SSD\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m dist = \u001b[43mmasked_ssd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_patch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatches_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# candidate-set selection\u001b[39;00m\n\u001b[32m    118\u001b[39m cand_idx, dmin = candidate_set(dist, eps)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[243]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mmasked_ssd\u001b[39m\u001b[34m(tgt_patch, patches_flat, mask)\u001b[39m\n\u001b[32m     61\u001b[39m tgt_flat = tgt_patch.reshape(tgt_patch.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m     62\u001b[39m masked_patches = patches_flat*mask\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches_flat\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_flat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m.sum(dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ---- Part 1 demo (will run after you implement the TODOs) ----\n",
    "OUT_H, OUT_W = 64, 64\n",
    "WINDOWS = [5]          # TODO (tuning): try e.g. [5, 11]\n",
    "EPS = 0.02            # TODO (tuning): try e.g. 0.02, 0.05, 0.1\n",
    "MAX_PATCHES = 50000     # e.g., 50000 to cap candidates for speed/memory\n",
    "RESIZE_SRC = 192       # resize textures for consistent runtime\n",
    "NEIGHBORHOOD = 4      # 4 or 8\n",
    "VERBOSE = False\n",
    "\n",
    "for path in texture_files[:3]:\n",
    "    src = load_texture_as_tensor(path, resize=RESIZE_SRC).to(device)\n",
    "    for w in tqdm(WINDOWS):\n",
    "        patches = extract_patches_from_image(src, window=w, max_patches=MAX_PATCHES, device=device)\n",
    "        out = efros_leung_synthesize_from_patch_pool(\n",
    "            patches=patches,\n",
    "            out_h=OUT_H, out_w=OUT_W,\n",
    "            eps=EPS,\n",
    "            neighborhood=NEIGHBORHOOD,\n",
    "            seed_fg_min=0,\n",
    "            border=0,\n",
    "            weighted=False,\n",
    "            h_mult=0.3,\n",
    "            verbose=VERBOSE,\n",
    "        )\n",
    "        show_grid([src.cpu(), out], nrow=2, resize_to=(OUT_H, OUT_W), title=f\"{path.name} | w={w}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48082f23",
   "metadata": {},
   "source": [
    "## (Required) Tuning / Ablations\n",
    "\n",
    "This assignment is *not* meant to be one-run-and-done. You should **tune** the key hyperparameters and report what changes.\n",
    "\n",
    "Minimum tuning expectations (include in the report):\n",
    "- **Part 1 (textures):** tune `window` size and `eps` (candidate-set threshold).\n",
    "- **Part 2 (MNIST):** tune `window` size and `eps` (and optionally `seed_fg_min`, `border`, `weighted`).\n",
    "- **Part 3 (PixelRNN):** tune at least one of: `hidden_dim`, `num_layers`, `dropout`, `lr`, `temperature`.\n",
    "\n",
    "Below is an optional helper scaffold to run small sweeps. Fill in the TODOs if you want\n",
    "a clean tuning workflow (recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def tune_efros_grid(\n",
    "    patch_pool_builder,\n",
    "    window_list,\n",
    "    eps_list,\n",
    "    out_h: int,\n",
    "    out_w: int,\n",
    "    trials: int = 1,\n",
    "    synth_kwargs: dict | None = None,\n",
    "    max_steps: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Grid-search over (window, eps) for Efros–Leung.\n",
    "\n",
    "    Args:\n",
    "      patch_pool_builder: function(window:int) -> patches (N,C,window,window) on `device`\n",
    "      window_list: list of odd window sizes to try, e.g. [5, 11]\n",
    "      eps_list: list of eps to try, e.g. [0.02, 0.05, 0.1]\n",
    "      trials: number of random restarts per config (average the score)\n",
    "      synth_kwargs: extra kwargs passed into efros_leung_synthesize_from_patch_pool\n",
    "      max_steps: optional cap on fill steps (useful for fast tuning)\n",
    "\n",
    "    Returns:\n",
    "      best_cfg: dict with keys {\"window\", \"eps\", \"score\"}\n",
    "      results: list of dicts (one per config), sorted by ascending score\n",
    "\n",
    "    TODO:\n",
    "      - For each (window, eps):\n",
    "          * build patch pool\n",
    "          * run efros_leung_synthesize_from_patch_pool(..., return_stats=True, max_steps=max_steps)\n",
    "          * use stats[\"avg_dmin\"] as a simple score (lower is better)\n",
    "          * average the score over `trials`\n",
    "      - Return the best config and a sorted result list.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0db3c",
   "metadata": {},
   "source": [
    "## Part 2 — MNIST (binarized)\n",
    "\n",
    "MNIST is downloaded automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e0661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binarized_mnist(threshold: float = 0.5):\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    train_set = torchvision.datasets.MNIST(root=\"/content/data\", train=True, download=True, transform=transform)\n",
    "    test_set  = torchvision.datasets.MNIST(root=\"/content/data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    x_train = train_set.data.unsqueeze(1).float() / 255.0\n",
    "    y_train = train_set.targets\n",
    "    x_test  = test_set.data.unsqueeze(1).float() / 255.0\n",
    "    y_test  = test_set.targets\n",
    "\n",
    "    x_train = (x_train > threshold).float()\n",
    "    x_test  = (x_test  > threshold).float()\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_binarized_mnist(threshold=0.5)\n",
    "print(\"MNIST:\", x_train.shape, y_train.shape, \"unique:\", torch.unique(x_train))\n",
    "print(\"foreground ratio (mean pixel):\", float(x_train.mean().item()))\n",
    "\n",
    "idx = torch.randperm(x_train.shape[0])[:64]\n",
    "show_grid([x_train[i,0] for i in idx], nrow=8, title=\"Binarized MNIST samples\", cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches_from_mnist_batch(\n",
    "    imgs: torch.Tensor,  # (B,1,28,28) in {0,1}\n",
    "    window: int,\n",
    "    max_patches: int,\n",
    "    device: torch.device = device,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Return patches: (N,1,window,window) sampled from the batch.\"\"\"\n",
    "    assert imgs.ndim == 4 and imgs.shape[1] == 1\n",
    "    unfold = torch.nn.Unfold(kernel_size=window, padding=0, stride=1)\n",
    "    patches = unfold(imgs)  # (B, win*win, L)\n",
    "    B, D, L = patches.shape\n",
    "    patches = patches.transpose(1, 2).reshape(B * L, 1, window, window)\n",
    "    if patches.shape[0] > max_patches:\n",
    "        idx = torch.randperm(patches.shape[0])[:max_patches]\n",
    "        patches = patches[idx]\n",
    "    return patches.to(device)\n",
    "\n",
    "# ---- Part 2 demo (will run after you implement the TODOs) ----\n",
    "# TODO (tuning): try different digits and hyperparameters; expect *stroke-like* outputs, not perfect digits.\n",
    "digit = 1 # Select Your Number\n",
    "window =               # TODO (tuning): e.g. 5, 7, 9\n",
    "num_source_images =   # TODO (tuning): more sources -> more variety, slower patch pool build\n",
    "max_patches =        # TODO (tuning): larger -> better coverage, slower matching\n",
    "\n",
    "# Optional knobs (not required for correctness)\n",
    "USE_MNIST_HEURISTICS = False  # set True to try a few stability tweaks on MNIST\n",
    "\n",
    "src_imgs = x_train[y_train == digit]\n",
    "sel = torch.randperm(src_imgs.shape[0])[:num_source_images]\n",
    "src_batch = src_imgs[sel].to(device)\n",
    "\n",
    "patch_pool = extract_patches_from_mnist_batch(src_batch, window=window, max_patches=max_patches, device=device)\n",
    "print(\"patch_pool:\", patch_pool.shape)\n",
    "\n",
    "# Core Efros–Leung parameters\n",
    "eps =                # TODO (tuning): smaller -> stricter matching, larger -> more randomness\n",
    "neighborhood =          # TODO\n",
    "\n",
    "# Heuristic parameters (off by default)\n",
    "seed_fg_min = 0\n",
    "border = 0\n",
    "weighted = False\n",
    "h_mult = 0.3\n",
    "if USE_MNIST_HEURISTICS:\n",
    "    seed_fg_min = 4\n",
    "    border = 2\n",
    "    weighted = True\n",
    "    h_mult = 0.25\n",
    "\n",
    "n_gen = 25\n",
    "samples = []\n",
    "for _ in range(n_gen):\n",
    "    gen = efros_leung_synthesize_from_patch_pool(\n",
    "        patches=patch_pool, out_h=28, out_w=28,\n",
    "        eps=eps, neighborhood=neighborhood,\n",
    "        seed_fg_min=seed_fg_min, border=border,\n",
    "        weighted=weighted, h_mult=h_mult,\n",
    "        verbose=False\n",
    "    )\n",
    "    samples.append(gen[0])\n",
    "\n",
    "show_grid(samples, nrow=5, title=f\"Efros–Leung MNIST generation (digit={digit}, window={window})\", cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a1c46",
   "metadata": {},
   "source": [
    "## Part 3 — PixelRNN (binarized MNIST)\n",
    "\n",
    "Implement a raster-scan autoregressive RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pixel_sequences(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    x: (B,1,28,28) in {0,1}\n",
    "    Returns:\n",
    "      x_in: (B,784) tokens in {0,1,2} where 2 is START\n",
    "      y:    (B,784) float targets in {0,1}\n",
    "    \"\"\"\n",
    "    B = x.shape[0]\n",
    "    pixels = x.view(B, -1).long()\n",
    "    start = torch.full((B, 1), 2, dtype=torch.long)\n",
    "    x_in = torch.cat([start, pixels[:, :-1]], dim=1)\n",
    "    y = pixels.float()\n",
    "    return x_in, y\n",
    "\n",
    "def make_dataloaders(x_train, x_test, batch_size=128, train_subset=None):\n",
    "    if train_subset is not None and train_subset < x_train.shape[0]:\n",
    "        idx = torch.randperm(x_train.shape[0])[:train_subset]\n",
    "        x_tr = x_train[idx]\n",
    "    else:\n",
    "        x_tr = x_train\n",
    "\n",
    "    x_in_tr, y_tr = make_pixel_sequences(x_tr)\n",
    "    x_in_te, y_te = make_pixel_sequences(x_test)\n",
    "\n",
    "    train_ds = TensorDataset(x_in_tr, y_tr)\n",
    "    test_ds = TensorDataset(x_in_te, y_te)\n",
    "\n",
    "    num_workers = 2 if device.type == \"cuda\" else 0\n",
    "    pin_memory = (device.type == \"cuda\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                              pin_memory=pin_memory, persistent_workers=(num_workers > 0))\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
    "                             pin_memory=pin_memory, persistent_workers=(num_workers > 0))\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39044745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelRNN(nn.Module):\n",
    "    def __init__(self, emb_dim=32, hidden_dim=256, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Tokens: 0, 1, START. (Use START = 2)\n",
    "        # Positions: row index in [0..27], col index in [0..27]\n",
    "        #\n",
    "        # TODO:\n",
    "        #   - token embedding: nn.Embedding(3, emb_dim)\n",
    "        #   - row/col embeddings: nn.Embedding(28, emb_dim) each\n",
    "        #   - LSTM: nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True, ...)\n",
    "        #   - output head: nn.Linear(hidden_dim, 1) to produce logits for x_t\n",
    "        #\n",
    "        # IMPORTANT: this must be a *causal* model. Do NOT peek at future pixels.\n",
    "        self.emb = None\n",
    "        self.row_emb = None\n",
    "        self.col_emb = None\n",
    "        self.lstm = None\n",
    "        self.out = None\n",
    "\n",
    "        # Optional convenience (recommended): precompute raster positions for length 784 (T=28*28)\n",
    "        # t = torch.arange(28 * 28)\n",
    "        # self.register_buffer(\"pos_row\", (t // 28).long(), persistent=False)\n",
    "        # self.register_buffer(\"pos_col\", (t % 28).long(), persistent=False)\n",
    "\n",
    "        # Remove this line after implementing the model.\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x_in: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x_in: (B,784) tokens in {0,1,2}, where 2 denotes START at t=0 (teacher-forcing input).\n",
    "        Return:\n",
    "          logits: (B,784) where logits[:,t] parameterizes p(x_t=1 | x_<t)\n",
    "\n",
    "        Notes:\n",
    "          - `make_pixel_sequences()` already inserts the START token and shifts the sequence.\n",
    "          - Add row/col positional embeddings based on t -> (t//28, t%28).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n: int, temperature: float = 1.0):\n",
    "        \"\"\"\n",
    "        Ancestral sampling in raster order.\n",
    "        Return: samples (n,1,28,28) in {0,1}.\n",
    "\n",
    "        TODO:\n",
    "          - Start with prev token = START for all n samples.\n",
    "          - For t=0..783:\n",
    "              - embed prev token (+ positional embedding for position t)\n",
    "              - run 1-step LSTM using cached hidden state\n",
    "              - map hidden -> logit -> prob\n",
    "              - sample next pixel using Bernoulli(prob)\n",
    "              - set prev = sampled pixel token (0/1)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nll_per_pixel(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    bce = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
    "    with torch.no_grad():\n",
    "        for x_in, y in loader:\n",
    "            x_in = x_in.to(device)\n",
    "            y = y.to(device)\n",
    "            logits = model(x_in)\n",
    "            loss = bce(logits, y)\n",
    "            total += float(loss.item())\n",
    "            count += int(y.numel())\n",
    "    return total / count\n",
    "\n",
    "def train_pixelrnn(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, epochs=10, lr=1e-3):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    bce = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "    history = {\"train_loss\": [], \"test_nll\": []}\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        n_batches = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"epoch {ep}/{epochs}\")\n",
    "        for x_in, y in pbar:\n",
    "            x_in = x_in.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x_in)\n",
    "            loss = bce(logits, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            running += float(loss.item())\n",
    "            n_batches += 1\n",
    "            pbar.set_postfix(loss=running / n_batches)\n",
    "\n",
    "        train_loss = running / max(1, n_batches)\n",
    "        test_nll = evaluate_nll_per_pixel(model, test_loader)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_nll\"].append(test_nll)\n",
    "        print(f\"epoch {ep}: train_loss={train_loss:.4f} | test_nll/pixel={test_nll:.4f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e4317",
   "metadata": {},
   "source": [
    "### PixelRNN tuning helper (optional but recommended)\n",
    "\n",
    "A simple hyperparameter sweep runner. Use a small `train_subset` and few epochs so you can iterate quickly.\n",
    "Then train your best configuration longer for the final results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_pixelrnn_configs(\n",
    "    x_train: torch.Tensor,\n",
    "    x_test: torch.Tensor,\n",
    "    configs: list[dict],\n",
    "    train_subset: int = 10000,\n",
    "    epochs: int = 2,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sweep a list of PixelRNN configs.\n",
    "\n",
    "    Each entry in `configs` is a dict like:\n",
    "      {\n",
    "        \"model_kwargs\": {\"emb_dim\": 32, \"hidden_dim\": 256, \"num_layers\": 2, \"dropout\": 0.1},\n",
    "        \"lr\": 1e-3,\n",
    "      }\n",
    "\n",
    "    TODO:\n",
    "      - For each config:\n",
    "          * make loaders with make_dataloaders(..., train_subset=train_subset)\n",
    "          * instantiate PixelRNN(**model_kwargs)\n",
    "          * train with train_pixelrnn(..., epochs=epochs, lr=lr)\n",
    "          * evaluate test_nll_per_pixel via evaluate_nll_per_pixel\n",
    "      - Return a list of results sorted by test_nll (ascending)\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58319a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Part 3 demo (will run after you implement PixelRNN) ----\n",
    "# TODO (tuning): change model size and training hyperparameters; report test NLL/pixel + sample quality.\n",
    "BATCH_SIZE = \n",
    "TRAIN_SUBSET = None   # e.g., 10000 for quick debugging / sweeps\n",
    "EPOCHS = \n",
    "LR =  # e.g.,1e-3\n",
    "\n",
    "MODEL_KWARGS = dict(\n",
    "    emb_dim= ,        # TODO (tuning)\n",
    "    hidden_dim= ,    # TODO (tuning)\n",
    "    num_layers= ,      # TODO (tuning)\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "TEMPERATURE = 1.0     # TODO (tuning)e.g.,0.7～1.3\n",
    "\n",
    "train_loader, test_loader = make_dataloaders(\n",
    "    x_train, x_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_subset=TRAIN_SUBSET,\n",
    ")\n",
    "\n",
    "model = PixelRNN(**MODEL_KWARGS).to(device)\n",
    "history = train_pixelrnn(model, train_loader, test_loader, epochs=EPOCHS, lr=LR)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(history[\"train_loss\"]) + 1), history[\"train_loss\"], marker=\"o\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"train BCE (mean)\")\n",
    "plt.title(\"PixelRNN training curve\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "samples = model.sample(n=64, temperature=TEMPERATURE)\n",
    "show_grid([samples[i, 0] for i in range(64)], nrow=8, title=\"PixelRNN samples (binary MNIST)\", cmap=\"gray\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
